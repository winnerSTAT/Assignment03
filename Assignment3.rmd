---
title: "DSC5103 Assignment 3"
subtitle: 'Logistic Regression'
author: "Tong Wang"
date: "Sep 2016"
output:
  html_document:
    highlight: tango
    theme: yeti
  pdf_document:
    highlight: zenburn
---
<!--
comments must be put in an HTML comment form
-->

```{r set-options, echo=FALSE, cache=FALSE}
options(width = 120)  # set output width
```

## NOTE:
This assignment is **due at 23:59 of Sep 15, Thursday**. You can work on this file directly and fill in your answers/code below. Please submit the output HTML file (name your file like G1Group02.html if you are from Group 02 of Section G1) onto IVLE/Files/Student Submission/Assignment3 folder.

Also, put the Section/Group and member info below.
```{r}
# Section G2
# Group 05
# Members: CHEN XIANG, XU YUWEN, YANG YUBO, ZHANG YUN
```

## Part I: Gender Discrimination in UC Berkeley Admissions
### Introduction
The *UCBAdmissions* dataset in R has aggregate data on applicants to graduate school at Berkeley for the six largest departments in 1973 classified by admission and sex. At issue is whether the data show evidence of sex bias in admission practices. There were 2691 male applicants, of whom 1198 (44.5%) were admitted, compared with 1835 female applicants of whom 557 (30.4%) were admitted. This gives a sample odds ratio of 1.83, indicating that males were almost twice as likely to be admitted.

Let's first convert the dataset into a dataframe.
```{r}
UCBAdmissions.df <- as.data.frame(UCBAdmissions)
head(UCBAdmissions.df)
```

We are going to use Logistic Regression to test the accusation.

### Questions

#### 1. Use the *reshape2* package to convert the dataset into proper shape with two separte columns showing the number of admitted and rejected applicants for each *Gender* and *Dept* combinations. (1 Mark)

Answer: 

```{r}
library(reshape2)
UCB <- dcast(UCBAdmissions.df, Gender + Dept ~ Admit, value.var = "Freq")
UCB
```

#### 2. Run Logistic Regression of *(admitted, rejected)* on predictor *Gender*. What is the probablity of a female being admitted? Briefly comment on whether there is sex bias based on the model output. (1 Mark)

Answer: 

```{r}
glm1 <- glm(cbind(Admitted, Rejected) ~ Gender, data = UCB, family = binomial())
summary(glm1)
# From the model perspective, gender is significant, and female has less chance to be admitted.
```


#### 3.  Run Logistic Regression of *(admitted, rejected)* on predictor *Gender* and *Dept*. Briefly comment on whether there is sex bias based on the model output and the difference from the conclusion made by the previous model. (1 Mark)

Answer: 

```{r}
glm2 <- glm(cbind(Admitted, Rejected) ~ Gender + Dept, data = UCB, family = binomial())
summary(glm2)

# By controling Departmant, we can see now gender is not so significant in the model, indicating that there is no gender bias statistically. The bias in previous model is maybe from the difference in Dept, and for male and female, they may have different preference in choosing between those departments.
```


#### 4.  Introduce interaction term between *Gender* and *Dept* into the previous model. Briefly interpret the model output. (1 Mark)

Answer: 

```{r}
glm3 <- glm(cbind(Admitted, Rejected) ~ Gender * Dept, data = UCB, family = binomial())
summary(glm3)
UCB$prob <- fitted.values(glm3)
DeptGender <- dcast(UCB, formula = Dept ~ Gender, value.var = "prob")
DeptGender
# From the table above, we can see that there is gender difference between each department, for Dept A, B, D and F, female are prefered, while in the others, male are prefered.
```


## Part II: . Logistic Regression on the mixture.example dataset
### Introduction
We have done k-Nearest Neighbour classification on the *mixture.example* dataset of the *ElemStatLearn* package. Here we want to do the same classification using Logistic Regression and compare their performance on the test dataset.

To save your time, below is copied from the previous *knn_demo.R* file with some minor modifications. You can simply continue from there.

```{r results='hide', message=FALSE, warning=FALSE}
library("ElemStatLearn")  # run install.packages("ElemStatLearn") if you haven't

# copy important ones out
x <- mixture.example$x
y <- mixture.example$y
prob <- mixture.example$prob
xnew <- mixture.example$xnew
px1 <- mixture.example$px1
px2 <- mixture.example$px2

# make dataframe for the training data (with x1, x2, and y)
df.training <- data.frame(x1=x[ , 1], x2=x[ , 2], y=y)
df.training$y <- as.factor(df.training$y)

# make dataframe for the "test" data (with xnew1, xnew2, and true prob, but not y!!)
df.grid <- data.frame(x1=xnew[ , 1], x2=xnew[ , 2])
df.grid$prob <- prob


# plot X and Y
library("ggplot2")
p0 <- ggplot() + geom_point(data=df.training, aes(x=x1, y=x2, color=y), size=4) + scale_color_manual(values=c("green", "red")) + theme_bw()

# add the true boundary into the plot
p.true <- p0 + stat_contour(data=df.grid, aes(x=x1, y=x2, z=prob), breaks=c(0.5))
p.true
```

The above plot is the true boundary from the dataset.

### Questions
#### 1. Run Logistic Regression of *y* on *x1* and *x2* using the *df.training* dataset. (1 Mark)

Answer: 

```{r}
lr1 <- glm(formula = y ~ x1 + x2, data = df.training, family = binomial())
summary(lr1)
```


#### 2. Predict the probability of *y* using *df.grid* as the newdata. Plot the decision boundary of model just like we did for the true decision boundary above. Interpret the boundary verbally. (1 Mark)

Answer: 

```{r}
df.grid$prob.lr1 <- predict(object = lr1, newdata = df.grid, type = "response")
# plot the decision boundary
p.lr1 <- p0 + stat_contour(data=df.grid, aes(x=x1, y=x2, z=prob.lr1), breaks=c(0.5))
p.lr1
# Graphically, the decision boundary is linear. The reason is that, basically, the model is log(p/1-p) = b0 + b1*x1 + b2*x2 and we cutoff at p=0.5, which means that log(p/1-p) is a constant, thus, the boundary is at where b0 + b1*x1 + b2*x2 is a constant. z = b0 + b1*x1 + b2*x2 is linear function, so definately the boundary is linear.
```


#### 3. Fit the Logistic Regression model with up to 6th-order polynomial of *x1* and *x2*. Repeat the prediction on *df.grid* and plot the decision boundary. (1 Mark)

Answer: 

```{r}
lr2 <- glm(formula = y ~ poly(x1, 6) + poly(x2, 6), data = df.training, family = binomial())
summary(lr2)
df.grid$prob.lr2 <- predict(object = lr2, newdata = df.grid, type = "response")
# plot the decision boundary
p.lr2 <- p0 + stat_contour(data=df.grid, aes(x=x1, y=x2, z=prob.lr2), breaks=c(0.5))
p.lr2
```


Next, let's generate a test dataset and compare the performance of the two logistic regression models with kNN. Again, we can copy the code from *mixture_knn.R*.
```{r results='hide', message=FALSE, warning=FALSE}
library("mvtnorm")
set.seed(123)
centers <- c(sample(1:10, 5000, replace=TRUE), 
             sample(11:20, 5000, replace=TRUE))
means <- mixture.example$means
means <- means[centers, ]
x.test <- rmvnorm(10000, c(0, 0), 0.2 * diag(2))
x.test <- x.test + means
y.test <- c(rep(0, 5000), rep(1, 5000))
df.test <- data.frame(x1=x.test[, 1], x2=x.test[, 2], y=y.test)

# best possible misclassification rate
bayes.error <- sum(mixture.example$marginal * (prob * I(prob < 0.5) + (1-prob) * I(prob >= 0.5)))
```
Here *x.test* and *y.test* are the separate test data for the *knn()* function, whereas *df.test* is for *glm()*. They are the same data in different format. The *bayes.error* gives the best possible misclassification rate when the true model is known. We will use it as the limit.

The following code obtains probability prediction of kNN for k=1, 7, and 100 and save the probability predictions as three columns in the *df.test* dataframe.
```{r}
## predict with various knn models
library("FNN")
ks <- c(1, 7, 100)
for (i in seq(along=ks)) {
    mod.test  <- knn(x, x.test, y, k=ks[i], prob=TRUE)
    prob <- attr(mod.test, "prob")
    prob <- ifelse(mod.test == "1", prob, 1 - prob)
    df.test[, paste0("prob.knn", ks[i])] <- prob
}
head(df.test)
```

#### 4. Using *df.test* as new data, obtain the probability prediction of the two Logistic Regression models built earlier, and save them as two columns in *df.test*, too. (1 Mark)

Answer: 

```{r}
df.test$prob.lr1 <- predict(object = lr1, newdata = df.test, type = "response")
df.test$prob.lr2 <- predict(object = lr2, newdata = df.test, type = "response")
head(df.test)
```


#### 5. Plot the misclassification rate of the 5 models against probability cutoff in one plot, and also plot *bayes.error* as the benchmark. (1 Mark)

Answer: 

```{r}
library("ROCR")
knn1.pred <- prediction(df.test$prob.knn1, df.test$y)
knn7.pred <- prediction(df.test$prob.knn7, df.test$y)
knn100.pred <- prediction(df.test$prob.knn100, df.test$y)
lr1.pred <- prediction(df.test$prob.lr1, df.test$y)
lr2.pred <- prediction(df.test$prob.lr2, df.test$y)

knn1.err <- performance(knn1.pred, measure = "err")
knn7.err <- performance(knn7.pred, measure = "err")
knn100.err <- performance(knn100.pred, measure = "err")
lr1.err <- performance(lr1.pred, measure = "err")
lr2.err <- performance(lr2.pred, measure = "err")

# plot all the errors
plot(knn1.err, col = "red", ylim = c(0.2, 0.55))
plot(knn7.err, col = "blue", add = TRUE)
plot(knn100.err, col = "green", add = TRUE)
plot(lr1.err, col = "orange", add = TRUE)
plot(lr2.err, col = "purple", add = TRUE)
# plot the benchmark
abline(h = bayes.error, col = "gray", lty = 2)

legend(x=0, y=0.3, legend=c("knn1.err", "knn7.err", "knn100.err", "lr1.err", "lr2.err", "bayes.error"), lty=c(1,1,1,1,1,1), col=c("red", "blue", "green", "orange", "purple", "gray"))
```


#### 6. Plot the ROC curve of all the 5 models in one plot, and compare the models. (1 Mark)

Answer: 

```{r}
knn1.ROC <- performance(knn1.pred, measure="tpr", x.measure="fpr")
knn7.ROC <- performance(knn7.pred, measure="tpr", x.measure="fpr")
knn100.ROC <- performance(knn100.pred, measure="tpr", x.measure="fpr")
lr1.ROC <- performance(lr1.pred, measure="tpr", x.measure="fpr")
lr2.ROC <- performance(lr2.pred, measure="tpr", x.measure="fpr")

plot(knn1.ROC, col = "red")
plot(knn7.ROC, col = "blue", add = TRUE)
plot(knn100.ROC, col = "green", add = TRUE)
plot(lr1.ROC, col = "orange", add = TRUE)
plot(lr2.ROC, col = "purple", add = TRUE)
abline(a=0, b=1, lty=2)

legend(x=0.8, y=0.2, legend=c("knn1.ROC", "knn7.ROC", "knn100.ROC", "lr1.ROC", "lr2.ROC", "bayes.error"), lty=c(1,1,1,1,1,1), col=c("red", "blue", "green", "orange", "purple", "gray"))

# compare the 5 models by checking their exact value of AUC
performance(knn1.pred, measure = "auc")@y.values
performance(knn7.pred, measure = "auc")@y.values
performance(knn100.pred, measure = "auc")@y.values
performance(lr1.pred, measure = "auc")@y.values
performance(lr2.pred, measure = "auc")@y.values
```
