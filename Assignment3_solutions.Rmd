---
title: "DSC5103 Assignment 3"
subtitle: 'Logistic Regression --- Solutions and Remarks'
author: "Tong Wang"
date: "Sep 2015"
output:
  html_document:
    highlight: tango
    theme: spacelab
---
<!--
comments must be put in an HTML comment form
-->

```{r set-options, echo=FALSE, cache=FALSE}
options(width = 120)  # set output width
```


## Q1. Gender Discrimination in UC Berkeley Admissions

The *UCBAdmissions* dataset in R has aggregate data on applicants to graduate school at Berkeley for the six largest departments in 1973 classified by admission and sex. At issue is whether the data show evidence of sex bias in admission practices. There were 2691 male applicants, of whom 1198 (44.5%) were admitted, compared with 1835 female applicants of whom 557 (30.4%) were admitted. This gives a sample odds ratio of 1.83, indicating that males were almost twice as likely to be admitted.

Let's first convert the dataset into a dataframe.
```{r}
UCBAdmissions.df <- as.data.frame(UCBAdmissions)
head(UCBAdmissions.df)
```

We are going to use Logistic Regression to test the accusation.

a. Use *reshape2* package to convert the dataset into proper shape with two separte columns showing the number of admitted and rejected applicants for each *Gender* and *Dept* combinations.

```{r}
library("reshape2")
data <- dcast(UCBAdmissions.df, Gender + Dept ~ Admit, value.var="Freq")
data
```

b. Run Logistic Regression of *(admitted, rejected)* on predictor *Gender*. What is the probablity of a female being admitted? Briefly comment on whether there is sex bias based on the model output.

```{r}
glm1 <- glm(cbind(Admitted, Rejected) ~ Gender, data, family = binomial())
summary(glm1)
```

The probability of admission for a female is `r predict(glm1, newdata=data.frame(Gender="Female"), type="response")`, whereas that for a male is `r predict(glm1, newdata=data.frame(Gender="Male"), type="response")`. These two numbers are simply the proportion of Male and Female in the dataset. The data says that there is less female admitted overall.

c. Run Logistic Regression of *(admitted, rejected)* on predictor *Gender* and *Dept*. Briefly comment on whether there is sex bias based on the model output and the difference from the conclusion made by the previous model.

```{r}
glm2 <- glm(cbind(Admitted, Rejected) ~ Gender + Dept, data, family = binomial())
summary(glm2)
```

After controling for *Dept*, the coefficient for dummy variable *GenderFemale* is now positive (`r coef(glm2)["GenderFemale"]`) but not significant, suggesting that there is no statistical difference between male and female.

**REMARK**: The discrepency between the two models is called Simpsonâ€™s Paradox. It is a paradox in which a correlation present in different group is reversed when the groups are combined. In the UCBAdmissions example, female tended to apply to competitive departments with low rates of admission even among qualified applicants, whereas male tended to apply to less competitive departments with high rates of admission among the qualified applicants. That makes the overall difference in admission rate.



d. Introduce interaction term between *Gender* and *Dept* into the previous model. Briefly interpret the model output.

```{r}
glm3 <- glm(cbind(Admitted, Rejected) ~ Gender * Dept, data, family = binomial())
summary(glm3)
```

Let's see the model's fitted probabilities.
```{r}
data$prob <- predict(glm3, type="response")
dcast(data, Dept ~ Gender, value.var="prob")
```
We can see actually females are favored by Dept A, B, D, and F.


## Q2. Logistic Regression on the mixture.example dataset

We have done k-Nearest Neighbour classification on the *mixture.example* dataset of the *ElemStatLearn* package. Here we want to do the same classification using Logistic Regression and compare their performance on the test dataset.

To save your time, below is copied from the previous *knn_demo.R* file with some minor modifications. You can simply continue from there.

```{r results='hide', message=FALSE, warning=FALSE}
library("ElemStatLearn")  # run install.packages("ElemStatLearn") if you haven't

# copy important ones out
x <- mixture.example$x
y <- mixture.example$y
prob <- mixture.example$prob
xnew <- mixture.example$xnew
px1 <- mixture.example$px1
px2 <- mixture.example$px2

summary(x)
summary(y)
summary(prob)

# make dataframe for x and y (for ggplot use)
df.training <- data.frame(x1=x[ , 1], x2=x[ , 2], y=y)
summary(df.training)
df.training$y <- as.factor(df.training$y)

# dataframe for plotting the boundary
df.grid <- expand.grid(x1=px1, x2=px2)
df.grid$prob <- prob
summary(df.grid)


# plot X and Y
library("ggplot2")
p0 <- ggplot() + geom_point(data=df.training, aes(x=x1, y=x2, color=y), size=4) + scale_color_manual(values=c("green", "red"))

# add the true boundary into the plot
p.true <- p0 + stat_contour(data=df.grid, aes(x=x1, y=x2, z=prob), breaks=c(0.5))
p.true
```

The above plot is the true boundary from the dataset.

a. Run Logistic Regression of *y* on *x1* and *x2* using the *df.training* dataset.
```{r}
lr1 <- glm(y ~ x1 + x2, data=df.training, family=binomial())
summary(lr1)
```


b. Predict the probability of *y* using *df.grid* as the newdata. Plot the decision boundary of model just like we did for the true decision boundary above. Interpret the boundary verbally.

```{r}
df.grid$prob.lr1 <- predict(lr1, newdata=df.grid, type="response")
p.lr1 <- p0 + stat_contour(data=df.grid, aes(x=x1, y=x2, z=prob.lr1), breaks=c(0.5)) 
p.lr1
```

**REMARK**: The boundary for Logistic Regression is linear! This is because for any cutoff for probability (we use $\bar{p}=0.5$ here) , there is a corresponding cutoff  $\bar{\eta} = \log \left( \frac{\bar{p}}{1 - \bar{p}} \right)$ for $\eta$. Recall that $\eta = \beta_0 + \beta_1 x_1 + \beta_2 x_2$. That means the decision boundary is simply those points satisfying $\hat{\beta}_0 + \hat{\beta}_1 x_1 + \hat{\beta}_2 x_2 = \bar{\eta}$, which is a line in the ($x_1, x_2$) plane.

c. Fit the Logistic Regression model with up to 6th-order polynomial of *x1* and *x2*. Repeat the prediction on *df.grid* and plot the decision boundary.

```{r}
## fit the Logistic Regression model with 6th-order polynomial
lr6 <- glm(y ~ poly(x1, 6) + poly(x2, 6), data=df.training, family=binomial())
summary(lr6)
df.grid$prob.lr6 <- predict(lr6, newdata=df.grid, type="response")
p.lr6 <- p0 + stat_contour(data=df.grid, aes(x=x1, y=x2, z=prob.lr6), breaks=c(0.5)) 
p.lr6
```

**REMARK**: One way to introduce nonlinear boundary is to introduce nonlinear (polynomial or logarithm) terms.

Next, let's generate a test dataset and compare the performance of the two logistic regression models with kNN. Again, we can copy the code from *knn_demo.R*.
```{r results='hide', message=FALSE, warning=FALSE}
library(MASS)
set.seed(123)
centers <- c(sample(1:10, 5000, replace=TRUE), 
             sample(11:20, 5000, replace=TRUE))
means <- mixture.example$means
means <- means[centers, ]
x.test <- mvrnorm(10000, c(0, 0), 0.2 * diag(2))
x.test <- x.test + means
y.test <- c(rep(0, 5000), rep(1, 5000))
df.test <- data.frame(x1=x.test[, 1], x2=x.test[, 2], y=y.test)


# best possible misclassification rate
bayes.error <- sum(mixture.example$marginal * (prob * I(prob < 0.5) + (1-prob) * I(prob >= 0.5)))
```
Here *x.test* and *y.test* are the separate test data for the *knn()* function, whereas *df.test* is for *glm()*. They are the same data in different format. The *bayes.error* gives the best possible misclassification rate when the true model is known. We will use it as the limit.

The following code obtains probability prediction of kNN for k=1, 7, and 100 and save the probability predictions as three columns in the *df.test* dataframe.
```{r}
## predict with various knn models
library("class")
ks <- c(1, 7, 100)
for (i in seq(along=ks)) {
    mod.test  <- knn(x, x.test, y, k=ks[i], prob=TRUE)
    prob <- attr(mod.test, "prob")
    prob <- ifelse(mod.test == "1", prob, 1 - prob)
    df.test[, paste0("prob.knn", ks[i])] <- prob
}
head(df.test)
```

d. Using *df.test* as new data, obtain the probability prediction of the two Logistic Regression models built earlier, and save them as two columns in *df.test*, too.

```{r}
df.test$prob.lr1 <- predict(lr1, newdata=df.test, type="response")
df.test$prob.lr6 <- predict(lr6, newdata=df.test, type="response")
head(df.test)
```

e. Plot the misclassification rate of the 5 models against probability cutoff in one plot, and also plot *bayes.error* as the benchmark.

Construct ROCR prediction objects first.
```{r}
library("ROCR")
lr1.pred <- prediction(df.test$prob.lr1, df.test$y)
lr6.pred <- prediction(df.test$prob.lr6, df.test$y)
knn1.pred <- prediction(df.test$prob.knn1, df.test$y)
knn7.pred <- prediction(df.test$prob.knn7, df.test$y)
knn100.pred <- prediction(df.test$prob.knn100, df.test$y)
```
Then plot misclassification rate.
```{r fig.width=6, fig.height=6}
lr1.err <- performance(lr1.pred, measure = "err")
lr6.err <- performance(lr6.pred, measure = "err")
knn1.err <- performance(knn1.pred, measure = "err")
knn7.err <- performance(knn7.pred, measure = "err")
knn100.err <- performance(knn100.pred, measure = "err")

plot(lr1.err, lwd=2, ylim=c(0.2, 0.5))
plot(lr6.err, lwd=2, add=TRUE, col="purple")
plot(knn1.err, add=TRUE, col="red")
plot(knn7.err, add=TRUE, col="green")
plot(knn100.err, add=TRUE, col="blue")
abline(h=bayes.error, lty=2)
```


f. Plot the ROC curve of all the 5 models in one plot, and compare the models.

```{r fig.width=6, fig.height=6}
lr1.ROC <- performance(lr1.pred, measure = "tpr", x.measure = "fpr")
lr6.ROC <- performance(lr6.pred, measure = "tpr", x.measure = "fpr")
knn1.ROC <- performance(knn1.pred, measure = "tpr", x.measure = "fpr")
knn7.ROC <- performance(knn7.pred, measure = "tpr", x.measure = "fpr")
knn100.ROC <- performance(knn100.pred, measure = "tpr", x.measure = "fpr")

plot(lr1.ROC, lwd=2)
plot(lr6.ROC, lwd=2, add=TRUE, col="purple")
plot(knn1.ROC, add=TRUE, col="red")
plot(knn7.ROC, add=TRUE, col="green")
plot(knn100.ROC, add=TRUE, col="blue")
abline(a=0, b=1, lty=2) # diagonal line
```

Let's also check the AUC.
```{r}
as.numeric(performance(lr1.pred, "auc")@y.values)
as.numeric(performance(lr6.pred, "auc")@y.values)
as.numeric(performance(knn1.pred, "auc")@y.values)
as.numeric(performance(knn7.pred, "auc")@y.values)
as.numeric(performance(knn100.pred, "auc")@y.values)
```

It seems the best kNN model (k=7) beats our Logistic Regression models. It is expected because of high nonlinearity in the data.


## [Optional] Poisson Regression on Affairs
In R package *COUNT*, there is a dataset *affairs*, recording the number of affairs reported by the participants in the past year together with a number of predictors.

```{r}
library("COUNT")
data("affairs")
summary(affairs)
```

Use Poisson Regression to fit the number of affairs (*naffairs*) and interpret the coefficients.

ANSWER: first construct a dataframe with *happy*, *religious*, and *years* converted into factors.
```{r}
data <- affairs[,1:2]
data$kids <- as.factor(data$kids)

dummies.happy <- as.matrix(affairs[,3:7])
dummies.religious <- as.matrix(affairs[,8:12])
dummies.years <- as.matrix(affairs[,13:18])
data$happy <- factor(dummies.happy %*% 1:ncol(dummies.happy), labels = colnames(dummies.happy))
data$religious <- factor(dummies.religious %*% 1:ncol(dummies.religious), labels = colnames(dummies.religious))
data$years <- factor(dummies.years %*% 1:ncol(dummies.years), labels = colnames(dummies.years))
summary(data)
```

Poisson regression:
```{r}
glm.poisson <- glm(naffairs ~ kids + happy + religious + years, family = poisson(), data = data)
summary(glm.poisson)
```

It seems we have overdispersion here...
